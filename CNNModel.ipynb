{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depedensi \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "from normalisasi import *\n",
    "from google_play_scraper import app, reviews #package library untuk scarape data review\n",
    "import pandas as pd #mengolah data\n",
    "from textblob import TextBlob #untuk Mengolahkata\n",
    "from googletrans import Translator\n",
    "\n",
    "# Download the NLTK corpora untuk menganalisis Sentiment dan support Textblob\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scarpping data Komen Tiktok beserta sentimentnya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the app package name\n",
    "app_package = 'com.twitter.android' # untuk mendapatkan package namenya ikutilah ling berikut ini\n",
    "# https://www.techmesto.com/find-android-app-package-name/\n",
    "\n",
    "# Get app details\n",
    "app_info = app(app_package)\n",
    "\n",
    "# Get reviews for the app\n",
    "result, continuation_token = reviews(\n",
    "    app_package,\n",
    "    lang='en',\n",
    "    country='us',\n",
    "    count=100,\n",
    ")\n",
    "\n",
    "# Create a DataFrame to store the reviews\n",
    "columns = ['Review', 'Sentiment']\n",
    "data = []\n",
    "\n",
    "# Populate the DataFrame with review data and sentiment analysis\n",
    "for review in result:\n",
    "    # Perform sentiment analysis using TextBlob\n",
    "    analysis = TextBlob(review['content'])\n",
    "    sentiment = 'Positive' if analysis.sentiment.polarity > 0 else 'Negative' if analysis.sentiment.polarity < 0 else 'Neutral'\n",
    "\n",
    "    row_data = [\n",
    "        review['content'],\n",
    "        sentiment,\n",
    "    ]\n",
    "    data.append(row_data)\n",
    "\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "csv_filename = f'{app_info[\"title\"]}_reviews_with_sentiment.csv'\n",
    "df.to_csv(csv_filename, index=False)\n",
    "\n",
    "print(f'DataFrame with sentiment analysis saved to {csv_filename}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translete to indonesia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_to_indonesian(text):\n",
    "    translator = Translator()\n",
    "    translation = translator.translate(text, dest='id')  # 'id' is the language code for Indonesian\n",
    "    return translation.text\n",
    "\n",
    "# Read the CSV file\n",
    "file_path = 'TikTok_reviews_with_sentiment.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Apply translation to the 'Text' column\n",
    "df['Review'] = df['Review'].apply(translate_to_indonesian)\n",
    "\n",
    "# Print the results\n",
    "output_file_path = 'TikTok_reviews_with_sentiment_ind.csv'\n",
    "df.to_csv(output_file_path, index=False)\n",
    "print(f\"\\nResults exported to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalisasi data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('TikTok_reviews_with_sentiment_ind.csv') #read Data\n",
    "\n",
    "texts = df['Review'].values\n",
    "labels = df['Sentiment'].values\n",
    "\n",
    "train_reviews = texts[:35000]\n",
    "train_sentiments = labels[:35000]\n",
    "test_reviews = texts[35000:]\n",
    "test_sentiments = labels[35000:]\n",
    "\n",
    "\n",
    "norm_train_reviews = pre_process_corpus(train_reviews)\n",
    "norm_test_reviews = pre_process_corpus(test_reviews)\n",
    "norm_train_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and preprocess the data\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(norm_train_reviews)\n",
    "sequences = tokenizer.texts_to_sequences(norm_train_reviews)\n",
    "X = pad_sequences(sequences, maxlen=100)\n",
    "\n",
    "print(\"Vocabulary size={}\".format(len(tokenizer.word_index)))\n",
    "print(\"Number of Documents={}\".format(tokenizer.document_count))\n",
    "tokenizer.word_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence Normalization\n",
    "Not all reviews are of same length. To handle this difference in length of reviews, we define a maximum length. For reviews which are smaller than this length, we pad them with zeros which longer ones are truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 1000  \n",
    "train_sequinces = tokenizer.texts_to_sequences(norm_train_reviews)\n",
    "test_sequences = tokenizer.texts_to_sequences(norm_test_reviews)\n",
    "# pad dataset to a maximum review length in words\n",
    "X_train = pad_sequences(train_sequinces, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_test = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding Labels\n",
    "The dataset contains labels of the form positive/negative. The following step encodes the labels using sklearn's LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Step 3: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, encoded_labels, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the Model\n",
    "Since textual data is a sequence of words, we utilize 1D convolutions to scan through the sentences. The model first transforms each word into lower dimensional embedding/vector space followed by 1d convolutions and then passing the data through dense layers before the final layer for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Build the CNN model\n",
    "embedding_dim = 50  # Dimensionality of the word embeddings\n",
    "filters = 128  # Number of filters in the convolutional layer\n",
    "kernel_size = 5  # Size of the filters\n",
    "hidden_dims = 100  # Number of neurons in the fully connected layer\n",
    "max_words = 10000  # Max number of words in the vocabulary\n",
    "max_len = 100  # Max number of words in a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len))\n",
    "model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(hidden_dims, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test))\n",
    "\n",
    "# Step 6: Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Evaluate the model\n",
    "y_pred_probabilities = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probabilities, axis=1)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display and save the classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display and save the classification report\n",
    "unique_classes = label_encoder.classes_.astype(str)  # Ensure classes are in string format\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "# Save the classification report to a CSV file\n",
    "class_report_dict = classification_report(y_test, y_pred, target_names=None, output_dict=True)\n",
    "class_report_df = pd.DataFrame(class_report_dict).transpose()\n",
    "class_report_df.to_csv('classification_report.csv')\n",
    "print(\"Classification Report saved to 'classification_report.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Versi Dua\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Step 8: Apply a threshold to convert probabilities to binary predictions\n",
    "threshold = 0.5  # You can adjust this threshold based on your problem\n",
    "y_pred_classes = (y_pred > threshold).astype(int).flatten()\n",
    "\n",
    "# Step 9: Decode the labels back to their original form\n",
    "decoded_pred_labels = label_encoder.inverse_transform(y_pred_classes)\n",
    "decoded_true_labels = label_encoder.inverse_transform(y_test)\n",
    "\n",
    "# Step 10: Create a DataFrame with original text, true labels, and predicted labels\n",
    "results_data = {\n",
    "    'Original_Text': tokenizer.sequences_to_texts(X_test),\n",
    "    'True_Labels': decoded_true_labels,\n",
    "    'Predicted_Labels': decoded_pred_labels\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "\n",
    "# Step 11: Save the results to a CSV file\n",
    "results_df.to_csv('results.csv', index=False)\n",
    "\n",
    "# Step 12: Print the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(decoded_true_labels, decoded_pred_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
